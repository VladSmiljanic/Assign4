---
title: "ECON 762 Assignment 4"
output: html_notebook
---

###Question 1

**a. Robustness**

Robustness is the ability to resist the injection of bad data values. An estimator should respond to these small number of bad data values with smoothness. That is to say, it should only respond gradually to the injection of bad data values or small changes in the model. A robust estimator should have broad application for it to be consider useful.

**b. Masking**

When outliers exist, using a non-robust method may result in the outliers going undetected due to the interaction of the method with the outliers. 

**c. Sensitivity Curve and Influence Curve**

Sensitivity curve displays whether an estimator is bounded when an outlier is inputted into the sample data. It displays the bias of the statistic when "bad data" is injected into the sample. The outlier can be an arbitrary number within a range of $\pm \infty$ and will help us determine the robustness of an estimator. An unbounded sensitivity curve is undesired as it results in a non-robust estimator. 

The influence function of an estimator is the asymptotic verson of the sensitivity curve. It is the approximation of an estimator as $n\to\infty$ when a small fraction of the sample , $\epsilon$, is identical outliers. A contamination neighbourhood that is a convex combination of the distribution used to produce the the asymptotic value of the estimator and a point mass at $x_0$. The influence function is the limit of the fraction of the convex distribution that is based on this point mass, as we approach the fraction from the right (ie $\epsilon >0$).

\begin{align*}
IF_{\hat\theta}(x_0,F)&=\lim_{\epsilon\downarrow0}\frac{\hat\theta_\infty\left((1-\epsilon)
F+\epsilon\delta_{x_0}\right)-\hat\theta_\infty(F)}{\epsilon}\\
\end{align*}

It may be conisdered the limit version of the sensitivity curve.

**d. Finite-Sample Breakdown Point**

The point at which including one additional 'corrupted' data point into a sample will result in the bias between the corrupted and uncorrupted samples to go to infinity. 

X is our sample and X' is our corrupted sample when replacing $m$ data points with arbitratary values (including $\pm \infty$ to ecompass extreme values). If our estimator $\hat \theta = f(X)$ then our finite-sampling breakdown point is:

\begin{align*}
\min\left\{\frac{m}{n};\hbox{bias}(m;f(X'),X)
    \hbox{ is infinite}\right\}
\end{align*}

When m=1, our breakdown point would be zero. Injecting one data point with an arbitrary value would have a large effect on $\hat \theta$ and cause the estimator to `break down`. The highest possible breakdown point is is one less then half the sample size or a breakdown point of 50%. 

**e. Huber (1964) M-Estimator**

Where estimating the median through the minimizing the absolute values provides for a robust (bounded) estimator it is not efficient due to discontunity problems at $\mu = 0$.The variance of the median is much higher than that of the mean and especially if no outliers present. As a result, the M-Estimator was proposed to combat this problem. It has the functional form:

\begin{align*}
\label{huber rho_c}
\rho_c(\mu)=\left\{\begin{array}{ll}
\mu^2&\text{ if } |\mu|\le c\\
2c|\mu|-c^2&\text{ if } |\mu|> c
\end{array}
\right..
\end{align*}

Where c will be the resistence parameter that is specified by the user. As $c\to 0$, the efficiency of the estimator decreases. The rule-of-thumb is $c=1.345\times s$ where s a robust measure of scale. This would allow the efficiency is almost as good as mean (905%) while also being robust.

###Question 2

a) The finite sample breakdown point 

###Question 3

a)

```{r}
set.seed(42)
n <- 100
x <- runif(n)
beta1 <- 1
beta2 <- 2
epsilon <- rnorm(n,sd=.1)
y <- beta1 + beta2*x + epsilon
model <- lm(y~x)
coef.sample<-coef(model)[2]

n.e0<-100
e0 <- seq(-1000,1000,length=n.e0)
sc.coef <- numeric(n.e0)
for(i in 1:n.e0) {
  e.augmented <- c(epsilon[1:99],e0[i])
  y <- beta1 + beta2*x + e.augmented
  sc.coef[i] <- coef(lm(y~x))[2]-coef.sample
}

## Sensitivity curve for the sample coef
plot(e0, sc.coef,
     ylab="Bias in coef()",
     xlab="$e_0$",
     type="l")
```

b) The key feature of the sensitivity curve is to help determine the robustness of an estimation method. We wish to have boundness in the sensitivity curve to determine if the estimator is robust and can handle injection of arbtirary data values. The OLS estimator has a sensitivity curve that is linear when injecting $x0$ over the pre-determined interval. This shows that our bias will go to infinity if we inject a data value of infinity. The OLS estimator is a non-robust estimator.

We should care because outliers can effect our estimator by large magintudes and producing a method where there is little confidence in the estimator.

###Question 4

a)
```{r}
set.seed(42)
require(robustbase)
n <- 100
x<-rnorm(n)
mean.x<-mean(x)
med.x<-median(x)
sd.x<-sd(x)
iqr.x<-IQR(x)
mad.x<-mad(x)
q.x<-Qn(x)
```

b)
```{r}
x0<-seq(from=-10,to=10,by=1)

vec.mean<-numeric(length(x0))
vec.med<-numeric(length(x0))
vec.sd<-numeric(length(x0))
vec.iqr<-numeric(length(x0))
vec.mad<-numeric(length(x0))
vec.q<-numeric(length(x0))

for (i in 1:length(x0)){
  x[101]<-x0[i]
  vec.mean[i]<-mean(x)-mean.x
  vec.med[i]<-median(x)-med.x
  vec.sd[i]<-sd(x)-sd.x
  vec.iqr[i]<-IQR(x)-iqr.x
  vec.mad[i]<-mad(x)-mad.x
  vec.q[i]<-Qn(x)-q.x
}

par(mfrow=c(2,3))
plot(x0,vec.mean,type="l")
plot(x0,vec.med,type="l")
plot(x0,vec.sd,type="l")
plot(x0,vec.iqr,type="l")
plot(x0,vec.mad,type="l")
plot(x0,vec.q,type="l")
```

c)

```{r}
set.seed(42)
require(robustbase)
n <- 20
x<-rnorm(n)
mean.x<-mean(x)
med.x<-median(x)
sd.x<-sd(x)
iqr.x<-IQR(x)
mad.x<-mad(x)
q.x<-Qn(x)

x0<-1000

vec.mean<-numeric()
vec.med<-numeric()
vec.sd<- numeric()
vec.iqr<-numeric()
vec.mad<-numeric()
vec.q<-numeric()

for (i in 1:10){
  for (j in 1:i){
    x[j]<-x0
  }
  vec.mean[i]<-mean(x)
  vec.med[i]<-median(x)
  vec.sd[i]<-sd(x)
  vec.iqr[i]<-IQR(x)
  vec.mad[i]<-mad(x)
  vec.q[i]<-Qn(x)
}

values<-data.frame(seq(1,10,by=1),vec.mean,vec.med,vec.sd,vec.iqr,vec.mad,vec.q)

colnames(values)<-c("M","Mean","Median","Standard Deviation","IQR","MAD","Q")

knitr::kable(values)
```

d)


###Question 5

###Question 6

The hat matrix H gives the fitted value of Y: $\hat Y = HY$. Our $X$ is a $n\times k$ matrix.

\begin{align*}
H&=X(X'X)^{-1}X'\\
HY&=X(X'X)^{-1}X'Y\\
HY&=X\hat\beta\\
HY&=\hatY\\
\end{align*}

The trace of the H matrix requires knowledge of the properties for finding the trace of a matrix:

\begin{align*}
tr(ABC) = tr(BAC) = tr(CBA)\\
\end{align*}

The trace of a matrix is the sum of the diagonals.

\begin{align*}
tr(X(X'X)^{-1}X') &= tr((X'X)^{-1}X'X)\\
tr(I_k)&=k
\end{align*}

An element of an H matrix is defined as $h_{ij}$. Since the trace of a matrix is the sum of diagonals, we can show that $\bar h = k/n$.

\begin{align*}
tr(H) &= \sum^k_{i=1} h_{ii} = k\\
\frac{1}{n}\sum^n_{i=1} h_{ii} &= \frac{1}{n}\bar h = \frac{k}{n}\\
\end{align*}

A hat matrix is symmetric and idempotent. We know that the property of an idempotent is that the a diagonal matrix is the squared sum of columns of the row the element is on. As a result of this property, this value will be $\ge 0$:

\begin{align*}
h_{ii} = \sum^n_{j=1} h^2_{ij} \ge 0 \\
\end{align*}

By decomposing the summation into diagonal and non-diagonal terms and dividing by $h_{ii}$, we can show that each diagonal matrix is $\le 0$:

\begin{align*}
\frac{h_{ii} }{h_{ii}}&= \frac{1}{h_{ii}}(h_{ii}^2+\sum_{j=1,j\ne i}^n h_{ij}^2)\\
1 &= h_{ii}+\frac{\sum_{j=1,j\ne i}^n h_{ij}^2}{h_{ii}} \\
h_{ii}&= 1 - \frac{\sum_{j=1,j\ne i}^n h_{ij}^2}{h_{ii}} \\
h_{ii}\le 1 \\
\end{align*}

Therefore, $0 \le h_{ii} \le 1$.

b)

###Question 7

