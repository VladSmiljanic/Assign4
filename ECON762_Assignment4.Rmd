---
title: "ECON 762 Assignment 4"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
  word_document: default
---

###Question 1

**a. Robustness**

Robustness is the ability to resist the injection of bad data values. An estimator should respond to these small number of bad data values with smoothness. That is to say, it should only respond gradually to the injection of bad data values or small changes in the model. A robust estimator should have broad applications.

**b. Masking**

When outliers exist, using a non-robust method may result in the outliers going undetected due to the interaction of the method with the outliers. These outliers would be "masking" themselves.

**c. Sensitivity Curve and Influence Curve**

Sensitivity curve displays whether an estimator is bounded when an outlier is inputted into the sample data. It displays the bias of the statistic when "bad data" is injected into the sample. The outlier can be an arbitrary number within a range of $\pm \infty$ and will help us determine the robustness of an estimator. An unbounded sensitivity curve is undesired as it reveals a non-robust estimator. 

The influence function of an estimator is the asymptotic verson of the sensitivity curve. It is the approximation of an estimator as $n\to\infty$ when a small fraction of the sample , $\epsilon$, are identical outliers. A contamination neighbourhood is a convex combination of the distribution used to produce the the asymptotic value of the estimator and a point mass at $x_0$. The influence function is the limit of the fraction of the convex distribution that is based on this point mass, as we approach the fraction from the right (ie $\epsilon >0$).

\begin{align*}
IF_{\hat\theta}(x_0,F)&=\lim_{\epsilon\downarrow0}\frac{\hat\theta_\infty\left((1-\epsilon)
F+\epsilon\delta_{x_0}\right)-\hat\theta_\infty(F)}{\epsilon}\\
\end{align*}

It may be conisdered the limit version of the sensitivity curve.

**d. Finite-Sample Breakdown Point**

The point at which including one additional 'corrupted' data point into a sample will result in the bias between the corrupted and uncorrupted samples go to infinity. 

X is our sample and X' is our corrupted sample when replacing $m$ data points with arbitratary values (including $\pm \infty$ to ecompass extreme values). If our estimator $\hat \theta = f(X)$ then our finite-sampling breakdown point is:

\begin{align*}
\min\left\{\frac{m}{n};\hbox{bias}(m;f(X'),X)
    \hbox{ is infinite}\right\}
\end{align*}

When m=1, our breakdown point would be zero. Injecting one data point with an arbitrary value would have a large effect on $\hat \theta$ and cause the estimator to `break down`. The highest possible breakdown point is is one less than half the sample size or a breakdown point of 50%. 

**e. Huber (1964) M-Estimator**

Where estimating the median through the minimizing the absolute values provides for a robust (bounded) estimator it is not efficient due to discontunity problems at $\mu = 0$.The variance of the median is much higher than that of the mean and especially if no outliers present. As a result, the M-Estimator was proposed to combat this problem. It has the functional form:

\begin{align*}
\label{huber rho_c}
\rho_c(\mu)=\left\{\begin{array}{ll}
\mu^2&\text{ if } |\mu|\le c\\
2c|\mu|-c^2&\text{ if } |\mu|> c
\end{array}
\right..
\end{align*}

Where c will be the resistence parameter that is specified by the user. As $c\to 0$, the efficiency of the estimator decreases. The rule-of-thumb is $c=1.345\times s$ where s a robust measure of scale. This would allow the efficiency is almost as good as mean (905%) while also being robust.

###Question 2

a) 

The finite sample breakdown point for the inter-quartile range would be $m=n/4-1$ and will have a breakdown point of 25% (expressed in the limit $n\to\infty$). Due to the ranked order nature of calculating the interquartile range, values that are $\pm \infty$ will fall outside of the interquartile range. As the inter-quartile range is the 75% quartile minues the 25% quartile, injecting a quarter of the sample with bad data will breach at least one point in the inter-quartile range, causing the bias to go to infinity. The interquartile range will have a breakdown point of 25%. Intuitively, it make senses because if we inject $m$ values that are less than 25% of the sample, they might all end up in the 25% quartile or above the 75% quartile, avoiding any bad data in the interquartile. A breakdown point of 25% would mean at least one bad data point would end up in the interquartile range. 

b) 

Where ranked order of the data points is used to calculate the interquartile range, $MAD_n$ is calculated by:

\begin{align*}
MAD_n = \frac{Med | x_i - Med(x_i)|}{0.675}
\end{align*}

We center all of the data points in a sample and center them about the medium of the sample. By finding the medium and dividing by 0.675, if our data sample is drawn from a Guassian distribution, $MAD_n$ will equal the standard deviation of the distribution. Where the interquartile range is based on the rank order of the sample, $MAD_n$ is based around the deviation around the median. This allows for the scale to be robust against injections of bad data and has a breakdown point of 50%.

c) 

Rousseeuq and Croux's $Q_n$ is based on distance similar to interquartile range. It is a more efficient robust scale estimator than $MAD_n$, which suffers a drawback of discountinity with it's influence function. If we look at the location framework for the sample median we see that it will suffer from this same discountinuity:
\begin{align*}
median {|x_i-x_j|;i<j}
\end{align*}

This will have a desired breakdownpoint of 50% but still suffers from inefficiency.$Q_n$ is a more efficient estimator of scale than the location framework produces while still have a 50% breakdown point: 

\begin{align*}
Q_n = d {|x_i-x_j|;i<j}_{(k)}
\end{align*}

Where $d$ is a constant factor and $k$ denotes the $k^{th}$ order statistic. We find the absolute distance between all combinations of variables. We can then make a set of values where $i<j$ and order the set in ascending order. We then choose the $k^{th}$ term in the order. To find $k$, we calculate $k={[n/2]+1\choose 2}$ where $(n/2)$ will be be the integer portion. If $n=3$ then $k={[n/2]+1\choose 2}={1+1\choose 2}=1$ and we would select the first term in the ordered set to equal $Q_n$. You would multiple it by $d=2.21914$ to normalize.$Q_n$ is robust and highly efficient with an explicit formula. 

###Question 3

a)

```{r}
set.seed(42)
n <- 100
x <- runif(n)
beta1 <- 1
beta2 <- 2
epsilon <- rnorm(n,sd=.1)
y <- beta1 + beta2*x + epsilon
model <- lm(y~x)
coef.sample<-coef(model)[2]

n.e0<-100
e0 <- seq(-1000,1000,length=n.e0)
sc.coef <- numeric(n.e0)
for(i in 1:n.e0) {
  e.augmented <- c(epsilon[1:99],e0[i])
  y <- beta1 + beta2*x + e.augmented
  sc.coef[i] <- coef(lm(y~x))[2]-coef.sample
}

## Sensitivity curve for the sample coef
plot(e0, sc.coef,
     ylab="Bias in coef()",
     xlab="$e_0$",
     type="l")
```

b) The key feature of the sensitivity curve is to help determine the robustness of an estimation method. We wish to have boundness in the sensitivity curve to determine if the estimator is robust and can handle injection of arbtirary data values. The OLS estimator has a sensitivity curve that is linear when injecting $x0$ over the pre-determined interval. This shows that our bias will go to infinity if we inject a data value of infinity. The OLS estimator is a non-robust estimator.

We should care because outliers can effect our estimator by large magnitudes and producing a method where there is little confidence in the estimator.

###Question 4

a)

```{r}
set.seed(42)
require(robustbase)
n <- 100
x<-rnorm(n)
mean.x<-mean(x)
med.x<-median(x)
sd.x<-sd(x)
iqr.x<-IQR(x)
mad.x<-mad(x)
q.x<-Qn(x)
```

b)

```{r}
set.seed(42)
require(robustbase)
n <- 100
x<-rnorm(n)
x0.seq<-seq(from=-10,to=10,by=1)

mean.x<-mean(x)
med.x<-median(x)
sd.x<-sd(x)
iqr.x<-IQR(x)
mad.x<-mad(x)
q.x<-Qn(x)

vec.mean<-numeric()
vec.med<-numeric()
vec.sd<-numeric()
vec.iqr<-numeric()
vec.mad<-numeric()
vec.q<-numeric()

for (i in 1:length(x0.seq)){
  x[101]<-x0.seq[i]
  vec.mean[i]<-mean(x)-mean.x
  vec.med[i]<-median(x)-med.x
  vec.sd[i]<-sd(x)-sd.x
  vec.iqr[i]<-IQR(x)-iqr.x
  vec.mad[i]<-mad(x)-mad.x
  vec.q[i]<-Qn(x)-q.x
}

par(mfrow=c(2,3))
plot(x0.seq,vec.mean,type="l")
plot(x0.seq,vec.med,type="l")
plot(x0.seq,vec.sd,type="l")
plot(x0.seq,vec.iqr,type="l")
plot(x0.seq,vec.mad,type="l")
plot(x0.seq,vec.q,type="l")
```

The median, interquartile range, $MAD_n$ and $Q_n$ appear to the be bounded.

c)

```{r}
set.seed(42)
require(robustbase)
n <- 20
x<-rnorm(n)
mean.x<-mean(x)
med.x<-median(x)
sd.x<-sd(x)
iqr.x<-IQR(x)
mad.x<-mad(x)
q.x<-Qn(x)

x0<-1000

vec.mean<-numeric()
vec.med<-numeric()
vec.sd<- numeric()
vec.iqr<-numeric()
vec.mad<-numeric()
vec.q<-numeric()

for (i in 1:10){
  x[1:i]<-x0
  vec.mean[i]<-mean(x)
  vec.med[i]<-median(x)
  vec.sd[i]<-sd(x)
  vec.iqr[i]<-IQR(x)
  vec.mad[i]<-mad(x)
  vec.q[i]<-Qn(x)
}

values<-data.frame(seq(1,10,by=1),vec.mean,vec.med,vec.sd,vec.iqr,vec.mad,vec.q)

colnames(values)<-c("M","Mean","Median","Standard Deviation","IQR","MAD","Q")

knitr::kable(values)
```

From looking at the table, the non-robust method values are much more sensitive to influence by the mass points. We can also see from the table, the breakdown points for the estimators with $Q_n$ being the only one to remain robust when influence exerted on half the sample.

d)

```{r}
set.seed(42)
require(robustbase)
n <- 20
x<-rnorm(n)

x0<-1000

for (i in 1:5){
  x[1:i]<-x0
  t.t<-(x-mean(x))/sd(x)
  print(subset(x,abs(t.t)>3.0))
  t.r<-(x-median(x))/mad(x)
  print(subset(x,abs(t.r)>3.0))
}


```

We can see that the non-robust three-sigma edit rule results in the value -2.656 being masked when compared to the robust method. Once we start increasing the number of point mass values in the sample, all of the point masses are masked when using the non-robust method for $m\ge2$. We can see that the non-robust method will resulting in masking as we inject bad data values into the sample. 

###Question 5

a) A box and whisker plot is a method that defines what points would be considered outliers and displays the median, interquartile range, the boundary for outliers and the outliers themselves. It defines 

```{r}
n <- 20
x<-rnorm(n)

boxplot(x)
```

The thick line is the median of the distribution while the top and bottom of the "box" are quartiles $Q_1$ and $Q_1$. The "whiskers" are the farthest points that are not outliers. Here they are defined as values within $(3/2)\times Q$. The points are data values that lie outside of the whiskers and values that are the same would be placed next to each other horizontally. 

b) The advantage for using a box and whisker plot is that it is a robust measurement of scale as it relies on the median values of the sample. Large data values that are

c) 

```{r}
set.seed(42)
require(robustbase)
n <- 20
x<-rnorm(n)

x0<-1000
bp<-boxplot(x)
print(bp$out)

for (i in 1:5){
  x[1:i]<-x0
  bp<-boxplot(x)
  print(bp$out)
}
```

###Question 6

a)

The hat matrix H gives the fitted value of Y: $\hat Y = HY$. Our $X$ is a $n\times k$ matrix.

\begin{align*}
H&=X(X'X)^{-1}X'\\
HY&=X(X'X)^{-1}X'Y\\
HY&=X\hat\beta\\
HY&=\hat Y\\
\end{align*}

The trace of the H matrix requires knowledge of the properties for finding the trace of a matrix:

\begin{align*}
tr(ABC) = tr(BAC) = tr(CBA)\\
\end{align*}

The trace of a matrix is the sum of the diagonals.

\begin{align*}
tr(X(X'X)^{-1}X') &= tr((X'X)^{-1}X'X)\\
tr(I_k)&=k
\end{align*}

An element of an H matrix is defined as $h_{ij}$. Since the trace of a matrix is the sum of diagonals, we can show that $\bar h = k/n$.

\begin{align*}
tr(H) &= \sum^k_{i=1} h_{ii} = k\\
\frac{1}{n}\sum^n_{i=1} h_{ii} &= \frac{1}{n}\bar h = \frac{k}{n}\\
\end{align*}

A hat matrix is symmetric and idempotent. We know that the property of an idempotent is that the a diagonal matrix is the squared sum of columns of the row the element is on. As a result of this property, this value will be $\ge 0$:

\begin{align*}
h_{ii} = \sum^n_{j=1} h^2_{ij} \ge 0 \\
\end{align*}

By decomposing the summation into diagonal and non-diagonal terms and dividing by $h_{ii}$, we can show that each diagonal matrix is $\le 0$:

\begin{align*}
\frac{h_{ii} }{h_{ii}}&= \frac{1}{h_{ii}}(h_{ii}^2+\sum_{j=1,j\ne i}^n h_{ij}^2)\\
1 &= h_{ii}+\frac{\sum_{j=1,j\ne i}^n h_{ij}^2}{h_{ii}} \\
h_{ii}&= 1 - \frac{\sum_{j=1,j\ne i}^n h_{ij}^2}{h_{ii}} \\
h_{ii}&\le 1 \\
\end{align*}

Therefore, $0 \le h_{ii} \le 1$.

b) 

We know that $X'X = \sum^n_{i=1}x_ix_i'$ and that $X_{(-t)}X'_{-t}$ subtracts the $t^{th}$ observation from $X'X$ matrix. Therefore:

\begin{align*}
X'_{(-t)}X_{(-t)} = X'X - x_tx'_t\\
\end{align*}

The Sherman-Morrison-Woodbury formula states $(A-UV')^{-1}=A^{-1}+A^{-1}U\left(I_m-V'A^{-1}U\right)^{-1}V'A^{-1}$ where $A$ is a $p \times p$ matrix and $U$ and $V$ are $p \times m$ matricies. From our equation above, we can state that $A = X'X$ and $U,V=x_t$ where m=1. We can use the Sherman-Morrison-Woodbury formula to solve for inverse of our equation:

\begin{align*}
(X_{(-t)}X'_({-t}))^{-1} &= (X'X - x_tx'_t)^{-1}\\
&= (X'X)^{-1} + (X'X)^{-1}x_t(1 -x'_t(X'X)^{-1}x_t)^{-1}x'_t(X'X)^{-1}\\
\end{align*}

We know that $h_{ii}=x_i'(X'X)^{-1}x_i$.

\begin{align*}
(X_{(-t)}X'_{(-t)})^{-1} &= (X'X)^{-1}+\frac{(X'X)^{-1}x_tx'_t(X'X)^{-1}}{(1 -h_{tt})}\\
\end{align*}

c) 

Similarily, $X'Y = \sum^n_{i=1}x'_iy_i$. We can find $X'_{(-t)}Y_{(-t)}$ by subtracting the $t^{th}$ observation $x_ty'_t$:

\begin{align*}
X'_{(-t)}Y_{(-t)} = X'Y - x'_ty_t\\
\end{align*}


d) 

Using the formula for OLS, we can estimate coefficients for data missing an observation

\begin{align*}
\hat\beta_{(-t)} = (X'_{(-t)}X_({-t}))^{-1}X'_{(-t)}Y_{(-t)}\\
\end{align*}

We can replace the elements with the values we previously determined.

\begin{align*}
\hat\beta_{(-t)} &= \bigg((X'X)^{-1}+\frac{(X'X)^{-1}x_tx'_t(X'X)^{-1}}{(1 -h_{tt})}\bigg)(X'Y - x'_ty_t)\\
&= (X'X)^{-1}X'Y + \frac{(X'X)^{-1}x_tx'_t(X'X)^{-1}}{(1 -h_{tt})}X'Y -(X'X)^{-1}x_ty_t-\frac{(X'X)^{-1}x_tx'_t(X'X)^{-1}}{(1 -h_{tt})}x_ty_t\\
\end{align*}

We know that $h_{ii}=x_i'(X'X)^{-1}x_i$ so we can simplify our expression.

\begin{align*}
&= \hat\beta + \frac{(X'X)^{-1}x_tx'_t\hat\beta}{(1 -h_{tt})} -\frac{(1-h_{tt})(X'X)^{-1}x_ty_t}{(1-h_{tt})}-\frac{(X'X)^{-1}x_th_{tt}y_t}{(1 -h_{tt})}\\
\end{align*}

The fitted values of an observation can be placed into equation ($\hat y = x'_t\hat\beta$) and we factor our common terms:

\begin{align*}
&= \hat\beta + \frac{(X'X)^{-1}x_t}{(1 -h_{tt})}(\hat y_t - (1-h_{tt})y_t - h_{tt}y_t)\\
&= \hat\beta + \frac{(X'X)^{-1}x_t}{(1 -h_{tt})}(\hat y_t - y_t)\\
\hat \beta_{(-t)}&= \hat\beta + \frac{(X'X)^{-1}x_t\hat \epsilon_t}{(1 -h_{tt})}\\
\hat \beta_{(-t)} &-\hat\beta = \frac{(X'X)^{-1}x_t\hat \epsilon_t}{(1 -h_{tt})}\\
\end{align*}